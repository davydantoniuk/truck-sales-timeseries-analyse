---
title: "Time series in the analysis of truck sales"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = FALSE,
	message = FALSE,
	warning = FALSE,
	keep_md =  FALSE
)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(ggfortify)
library(lmtest)
library(forecast)
library(stringr)
library(zoo)
library(knitr)
library(lmtest)
library(tseries)
library(nortest)
```

## Data

```{r}
data<-read.csv("Truck_sales.csv")
data_copy <- data
colnames(data_copy) <- c("YearMonth","Sales") 
kable(t(head(data_copy)))
```

```{r}
data$Year <- as.numeric(paste0('20',str_sub(data$Month.Year, start = 1 , end =2)))  
data$Month <- str_sub(data$Month.Year, start = 4, end = 6)
data$YearMonth <- paste(data$Year, data$Month, sep = "-")
```

The following dataset shows monthly truck sales data from 2003 to 2014. It includes information on the number of trucks sold in each month.

## Basic statistics

```{r}
basic_stats <- data.frame(
  Metric = c("N obs.", "Średnia", "Odch. stand.", "Min", 
             "1 kwartyl", "Mediana", "3 kwartyl", "Max"),
  Value = c(
    nrow(data),                       
    mean(data$Number_Trucks_Sold, na.rm = TRUE),  
    sd(data$Number_Trucks_Sold, na.rm = TRUE),    
    min(data$Number_Trucks_Sold, na.rm = TRUE), 
    quantile(data$Number_Trucks_Sold, 0.25, na.rm = TRUE),
    median(data$Number_Trucks_Sold, na.rm = TRUE),          
    quantile(data$Number_Trucks_Sold, 0.75, na.rm = TRUE),  
    max(data$Number_Trucks_Sold, na.rm = TRUE)              
  )
)
transposed_stats <- as.data.frame(t(basic_stats))
colnames(transposed_stats) <- basic_stats$Metric
transposed_stats <- transposed_stats[-1, ]  
kable(transposed_stats, align = "c")
```

## Data visualization

### Basic charts

```{r}
library(ggplot2)
ggplot(data, aes(x = factor(YearMonth, levels = unique(YearMonth)), y = Number_Trucks_Sold)) +
  geom_line(color = "blue", group = 1) +  
  geom_point() + 
  labs(title = "Sprzedaż ciężarówek w czasie",
       x = "Rok",
       y = "Liczba sprzedanych ciężarówek") +
  scale_x_discrete(labels = ifelse(data$Month == "Jan", data$Year, "")) +  
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5, size = 10))  
```

**Growth trend:**The number of trucks sold has steadily increased from 2003 to 2014, indicating that the market is growing.
**Seasonality:** Sales show clear seasonality with regular ups and downs in certain months, such as December or January.**Variability:**The difference between the highest and lowest sales during the year increased significantly between 2010 and 2014 compared to the 2003-2006 period.**Local declines:** Periodic declines in sales (e.g. 2009-2010) may be due to external factors such as the financial crisis.**Maximum sales:**The highest sales were recorded at the end of 2014, confirming the dynamic development of the market.

```{r}
monthly_sales <- aggregate(Number_Trucks_Sold ~ Month, data, mean)
monthly_sales$Month <- factor(monthly_sales$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))

ggplot(monthly_sales, aes(x = Month, y = Number_Trucks_Sold)) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(title = "Średnia sprzedaż ciężarówek w podziale na miesiące",
       x = "Miesiąc",
       y = "Średnia sprzedaż") +
  theme_minimal()
```

**Seasonality of sales:** The highest average sales occur during the summer (May-August), suggesting that summer is the 
most active time in the industry. **Best months:** July and August lead the way in terms of average sales, which may be 
due to companies preparing before the end of the third quarter.**Lower sales:** The lowest results occur in January and 
February, which can be linked to the New Year period and lower investment activity.**Slow growth:** From March to May, 
sales rise steadily, peaking in the summer and then falling in September and October, indicating a seasonal slowdown 
after the vacations.

```{r}
print(ggplot(data, aes(y = Number_Trucks_Sold)) +
  geom_boxplot(fill = "lightblue", color = "black") + 
  labs(title = "Rozkład sprzedaży ciężarówek (wszystkie lata)",
       y = "Liczba sprzedanych ciężarówek",
       x = "") +
  theme_minimal() +
  theme(axis.text.x = element_blank(),  
        axis.ticks.x = element_blank()))
```

**Median sales:** The median (the line in the middle of the box) is about 500 trucks sold, which means that half the months had higher sales and the other half had lower sales.**Interquartile range (IQR):** Most of the data (50%) is in the range of about 400 to 600 trucks sold, indicating relatively little variability in sales in most months.
**No outliers:** There are no outlier points on the chart, suggesting that sales are fairly uniform and no extreme low or high values. **Range of values:** The box plot shows that the minimum sales are around 250 trucks and the maximum sales are around 750 trucks per month.

```{r message=FALSE, warning=FALSE}
yearly_sales <- aggregate(Number_Trucks_Sold ~ Year, data, sum)
full_years <- data.frame(Year = 2003:2014)
yearly_sales <- merge(full_years, yearly_sales, by = "Year", all.x = TRUE)
yearly_sales$Number_Trucks_Sold[is.na(yearly_sales$Number_Trucks_Sold)] <- 0
ggplot(yearly_sales, aes(x = Year, y = Number_Trucks_Sold)) +
  geom_bar(stat = "identity", fill = "orange") +
  labs(title = "Całkowita sprzedaż ciężarówek w podziale na lata",
       x = "Rok",
       y = "Całkowita sprzedaż") +
  scale_x_continuous(breaks = 2003:2014) +
  theme_minimal()
```

**Constant sales growth:** Total truck sales have been growing year-on-year in an almost uninterrupted manner, 
indicating the market's development from 2003 to 2014.**Slower growth from 2009 to 2010:** Since 2011, a clear 
accelerated growth in sales has been evident, culminating in 2014, the best year in the period under review.

```{r message=FALSE, warning=FALSE}
monthly_sales <- aggregate(Number_Trucks_Sold ~ Month + Year, data, mean)
monthly_sales$Month <- factor(monthly_sales$Month, levels = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
ggplot(monthly_sales, aes(x = Month, y = Number_Trucks_Sold, group = Year, color = factor(Year))) +
  geom_line() +
  labs(title = "Średnia miesięczna sprzedaż w podziale na lata",
       x = "Miesiąc",
       y = "Średnia sprzedaż",
       color = "Rok") +
  theme_minimal()
```

**Seasonality of sales:** Each year, sales rise in the first half of the year, reaching a peak in the summer months (June-July), and then fall in the second half of the year, reaching their lowest values in November and December.
**Stable pattern across years:** The overall shape of the curves is very similar for all years, indicating a clear and repeatable pattern of seasonality.**Growth in sales over the years:** The higher lines for 2013-2014 indicate an overall increase in sales in those years compared to the beginning of the period (2003-2006). Particularly evident is the increase in sales during the summer months.

### Time series analysis

```{r}
# Preparing data for time series analysis
month_mapping <- c("Jan" = "01", "Feb" = "02", "Mar" = "03", "Apr" = "04",
                   "May" = "05", "Jun" = "06", "Jul" = "07", "Aug" = "08",
                   "Sep" = "09", "Oct" = "10", "Nov" = "11", "Dec" = "12")
data$Month <- month_mapping[data$Month]
data$YearMonth <- paste(data$Year, data$Month, sep = "-")
data$YearMonth <- as.Date(paste0(data$YearMonth, "-01"))
data <- data[, !(names(data) %in% c("Month.Year","Month", "Year"))]
colnames(data) <- c("Sales", "YearMonth")
data <- data[, c("YearMonth", "Sales")]
```

```{r}
szereg <- zoo(data$Sales, order.by = data$YearMonth)
# tsdisplay(szereg,col=2,lwd=2,las=1)
par(mfrow = c(1, 2))
acf(ts(data$Sales, frequency = 12), col = 2, lwd = 2, main = "Autocorrelation (ACF)")
pacf(ts(data$Sales, frequency = 12), col = 2, lwd = 2, main = "Partial Autocorrelation (PACF)")
par(mfrow = c(1, 1))
```

The graph shows ACF and PACF. ACF declines slowly, indicating a minor role for the MA component, while the sharp decline in PACF suggests a greater influence of the AR component.

```{r}
plot(stl(ts(data$Sales, frequency = 12),s.window="periodic"),col=2,lwd=2)
```

The graph shows the decomposition of the time series using the STL method. The seasonal component confirms the presence of regular patterns, the trend indicates long-term growth, and the residual component contains random fluctuations. The data can be described taking into account both trend and seasonality.

### Stationarity of the time series

In the previous graph, you can see that the time series has an upward trend and seasonality, which means that it is not stationary. So, we need to do differentiation to make it stationary. So we will do a first-order differentiation we will do a visualization and check it with appropriate tests.

```{r}
diff_szereg <- diff(szereg)
plot(diff_szereg, col = 2, lwd = 2)
```

In the given graph you can see that the first-order variation made the time series stationary. Now we will check it with the appropriate tests.

```{r}
adf_test <- adf.test(diff_szereg)
kpss_test <- kpss.test(diff_szereg)
pp_test <- pp.test(diff_szereg)
test_results <- data.frame(
  Nazwa_testu = c("Augmented Dickey-Fuller", "Kwiatkowski-Phillips-Schmidt-Shin", "Phillips-Perron"),
  Statystyka = c(adf_test$statistic, kpss_test$statistic, pp_test$statistic),
  p_value = c(adf_test$p.value, kpss_test$p.value, pp_test$p.value)
)
kable(test_results)
```

Three tests supported the stationarity of the time series.

```{r message=FALSE, warning=FALSE}
## Train test split
train_size <- nrow(data) - 12
train_szereg <- szereg[1:train_size]
test_szereg <- szereg[(train_size + 1):nrow(data)]
train_ts <- ts(coredata(train_szereg), frequency = 12, 
               start = c(as.numeric(format(start(train_szereg), "%Y")), 
                         as.numeric(format(start(train_szereg), "%m"))))
test_ts <- ts(coredata(test_szereg), frequency = 12,
              start = c(as.numeric(format(start(test_szereg), "%Y")), 
                        as.numeric(format(start(test_szereg), "%m"))))
```

## Holt-Winters

```{r echo=TRUE}
model_holtwinters <- HoltWinters(train_ts,seasonal="multiplicative")
```

The parameter `seasonal=“multiplicative”` takes into account seasonality, the amplitude of which changes in proportion to the level of the trend, which fits data with an increasing trend and intensifying seasonality.

```{r}
forecast_holtwinters <- forecast(model_holtwinters, h = 12)
x_range <- range(time(train_ts), time(test_ts), time(forecast_holtwinters$mean))
y_range <- range(train_ts, forecast_holtwinters$mean, test_ts)
plot(train_ts, col = 5, lwd = 4, xlim = x_range, ylim = y_range, 
     main = "Holt-Winters method forecasting", xlab = "Czas", ylab = "Wartości")
lines(forecast_holtwinters$mean, col = 4, lwd = 3)
lines(test_ts, col = 2, lwd = 2,alpha=0.5)
legend("topleft", legend = c("Training data", "Forecast", "Test data"), col = c(5, 4, 2), lwd = 2)
```

The forecast shows seasonality and trend in the training data well, which means that the method has correctly captured patterns in the data. Comparing the forecast with the test data shows that the model works well, although there may be minor differences.

### Checking the assumptions

```{r message=FALSE, warning=FALSE}
# Calculation of residuals
residuals_hw <- residuals(model_holtwinters)
fitted_hw <- fitted(model_holtwinters)
# Normality of the residuals
shapiro <- shapiro.test(residuals_hw)
ad <- ad.test(residuals_hw)
# Homogeneity of variance
bp <- bptest(residuals_hw~fitted_hw)
gq <- gqtest(residuals_hw~fitted_hw)

test_results_holt <- data.frame(
  Nazwa_testu = c(
    "Shapiro-Wilk", "Anderson-Darling",
    "Breusch-Pagan", "Goldfeld-Quandt"
  ),
  Statystyka = c(
    shapiro$statistic, ad$statistic,
    bp$statistic, gq$statistic
  ),
  p_value = c(
    shapiro$p.value, ad$p.value,
    bp$p.value, gq$p.value
  )
)
kable(test_results_holt)
```

In the tests performed, we can see that the assumption of normality of errors has been met, because in both tests the p-values are greater than 0.05. The assumption of homogeneity of variance has been violated, because in the first test the p-value is less than 0.05, which forces us to reject the null hypothesis, although and Goldfeld-Quandt test says to accept it.

## Polynomial regression

```{r}
aic_values <- numeric(length(1:25))
for (degree in 1:25) {
  model <- lm(szereg ~ poly(1:length(szereg), degree))
  aic_values[degree] <- AIC(model)
}
plot(1:25, aic_values, type = "b", xlab = "Polynomial Degree", ylab = "AIC",
     main = "AIC w zależnosci od stopnia wielomianu")
```

The graph shows that the best degree of the polynomial is 24, since it has the lowest AIC score.

```{r}
model_poly <- lm(szereg ~ poly(1:144, 24))
forecast_poly <- predict(model_poly,newdata = list(1:144))
```

```{r}
forecast_poly_ts <- ts(
  coredata(forecast_poly[133:144]), 
  frequency = 12,
  start = c(as.numeric(format(start(test_szereg), "%Y")), 
            as.numeric(format(start(test_szereg), "%m")))
)
x_range <- range(time(train_ts), time(test_ts), time(forecast_poly_ts))
y_range <- range(c(train_ts, forecast_poly_ts, test_ts))
plot(train_ts, col = 5, lwd = 4,
     main = "Forecasting by polynomial regression method", xlab = "Czas", ylab = "Wartości",
     xlim = x_range, ylim = y_range)
lines(forecast_poly_ts, col = 4, lwd = 3)
lines(test_ts, col = rgb(1, 0, 0, alpha = 0.5), lwd = 2)
legend("topleft", legend = c("Training data", "Forecast", "Test data"), col = c(5, 4, 2), lwd = 2)
```

The forecast reproduces the test data reasonably well, accounting for both seasonality and the overall trend of the data. However, compared to the Holt-Winters method, polynomial regression performs slightly worse in terms of forecast accuracy, especially for more complex seasonal patterns.

### Diagnostic charts 

```{r}
autoplot(model_poly)
```

The diagnostic charts indicate that the polynomial regression model may have some problems. The “Residuals vs Fitted” graph shows curvature, suggesting that the model has not fully captured the structure of the data. The “Normal Q-Q” graph shows that the residuals are not perfectly normally distributed, especially at the ends, indicating a possible deviation from the assumption of normality. “Scale-Location” shows an increasing variance of the residuals, which may suggest a heteroskedasticity problem. The “Residuals vs Leverage” chart identifies several points of high influence (high leverage) that may significantly affect the model fit.

### Checking the assumptions

```{r message=FALSE, warning=FALSE}
# Calculation of residuals
residuals_poly <- residuals(model_poly)
residuals_poly <- coredata(residuals_poly)
fitted_poly <- fitted(model_poly)
# Normality of the residuals
shapiro <- shapiro.test(residuals_poly)
ad <- ad.test(residuals_poly)
# Jednorodność wariancji
bp <- bptest(residuals_poly~fitted_poly)
gq <- gqtest(residuals_poly~fitted_poly)
# Autocorrelation
dw <- dwtest(model_poly)
bg <- bgtest(model_poly)
test_results <- data.frame(
  Nazwa_testu = c(
    "Shapiro-Wilk", "Anderson-Darling",
    "Breusch-Pagan", "Goldfeld-Quandt",
    "Durbin-Watson", "Breusch-Godfrey"
  ),
  Statystyka = c(
    shapiro$statistic, ad$statistic,
    bp$statistic, gq$statistic,
    dw$statistic, bg$statistic
  ),
  p_value = c(
    shapiro$p.value, ad$p.value,
    bp$p.value, gq$p.value,
    dw$p.value, bg$p.value
  )
)
kable(test_results)
```

Test results indicate that the residuals of the polynomial regression model have a normal distribution. The assumption of homogeneity of variance was violated, as both tests have p-values less than 0.05. Tests for autocorrelation indicate that the residuals are correlated, which is a good signal for time series analysis.

## SARIMA

```{r echo = TRUE}
sarima <- auto.arima(train_ts, seasonal = TRUE, stepwise = FALSE, approximation = FALSE)
summary(sarima)
```

This code snippet uses the `auto.arima` function from the `forecast` package to automatically construct the `SARIMA` model.
The SARIMA model includes both autoregressive $AR$ and seasonal $SAR$ components adjusted for data with pronounced trend and seasonality. The coefficients of the model indicate a moderate effect of ordinary and seasonal lags on the time series. The model has a good fit, as evidenced by low error values and low autocorrelation of residuals($ACF1 = 0.0073$).
The `SARIMA` model, based on the estimated parameters, can be written in the form:
$$(1 + 0.2851 B)(1 + 0.0826 B^{12} - 0.2318 B^{24})(1 - B)(1 - B^{12}) Y_t = \epsilon_t$$.

```{r}
forecast_sarima <- forecast(sarima, h = 12)
x_range <- range(time(train_ts), time(test_ts), time(forecast_sarima$mean))
y_range <- range(train_ts, forecast_sarima$mean, test_ts)
plot(train_ts, col = 5, lwd = 4, xlim = x_range, ylim = y_range, 
     main = "SARIMA model forecast", xlab = "Czas", ylab = "Wartości")
lines(forecast_sarima$mean, col = 4, lwd = 2)
lines(test_ts, col = 2, lwd = 2)
legend("topleft", legend = c("Training data", "Forecast", "Test data"), 
       col = c(5, 4, 2), lwd = 2)
```

The graph shows a time series forecast made using the SARIMA model. The forecast reproduces the test data well, taking into account both trend and seasonality, which shows the effectiveness of the model in analyzing this type of data.

### Checking the assumptions

```{r message=FALSE, warning=FALSE}
# Calculation of residuals
residuals_sarima <- residuals(sarima)
residuals_sarima <- coredata(residuals_sarima)
fitted_values <- fitted(sarima)
# Normality of the residuals
shapiro <- shapiro.test(residuals_sarima)
ad <- ad.test(residuals_sarima)
# Homogeneity of variance
bp <- bptest(residuals_sarima ~ fitted_values)
gq <- gqtest(residuals_sarima ~ fitted_values)

test_results_sarima <- data.frame(
  Nazwa_testu = c(
    "Shapiro-Wilk", "Anderson-Darling",
    "Breusch-Pagan", "Goldfeld-Quandt"
  ),
  Statystyka = c(
    shapiro$statistic, ad$statistic,
    bp$statistic, gq$statistic
  ),
  p_value = c(
    shapiro$p.value, ad$p.value,
    bp$p.value, gq$p.value
  )
)
kable(test_results_sarima)
```

The assumption of normality of the residuals was violated because the p-value is less than 0.05 in the second test. The assumption of homogeneity of variance was also violated because the p-values are less than 0.05 in both tests.


